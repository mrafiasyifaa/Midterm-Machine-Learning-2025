{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transaction Amount Prediction using Regression Models\n",
        "\n",
        "## Objective\n",
        "This notebook implements an end-to-end machine learning **regression pipeline** to predict transaction amounts. We will:\n",
        "1. Perform Exploratory Data Analysis (EDA)\n",
        "2. Preprocess and clean the data\n",
        "3. Engineer relevant features\n",
        "4. Train multiple regression models\n",
        "5. Evaluate and compare model performance using regression metrics\n",
        "\n",
        "## Dataset\n",
        "- **Source**: Credit card transaction data\n",
        "- **Target**: `TransactionAmt` (continuous value - amount of transaction)\n",
        "- **Features**: Various transaction, card, and customer-related features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "# Machine Learning libraries\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Regression models\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    mean_absolute_percentage_error\n",
        ")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the training data\n",
        "train_df = pd.read_csv('datasets/transaction/train_transaction.csv')\n",
        "\n",
        "print(\"Dataset Shape:\", train_df.shape)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"First 5 rows:\")\n",
        "print(train_df.head())\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Target Variable Statistics (TransactionAmt):\")\n",
        "print(train_df['TransactionAmt'].describe())\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Missing Values:\")\n",
        "print(f\"Total missing values: {train_df.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Transaction Amount Distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(train_df['TransactionAmt'], bins=100, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Transaction Amount Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Transaction Amount ($)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_xlim(0, 1000)  # Focus on common range\n",
        "\n",
        "# Box plot\n",
        "axes[1].boxplot(train_df['TransactionAmt'], vert=True)\n",
        "axes[1].set_title('Transaction Amount Box Plot', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Transaction Amount ($)')\n",
        "axes[1].set_ylim(0, 500)\n",
        "\n",
        "# Log scale distribution\n",
        "axes[2].hist(np.log1p(train_df['TransactionAmt']), bins=100, color='lightcoral', edgecolor='black')\n",
        "axes[2].set_title('Log-Transformed Transaction Amount', fontsize=14, fontweight='bold')\n",
        "axes[2].set_xlabel('Log(Transaction Amount)')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTransaction Amount Range: ${train_df['TransactionAmt'].min():.2f} - ${train_df['TransactionAmt'].max():.2f}\")\n",
        "print(f\"Mean Transaction Amount: ${train_df['TransactionAmt'].mean():.2f}\")\n",
        "print(f\"Median Transaction Amount: ${train_df['TransactionAmt'].median():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a working copy\n",
        "df = train_df.copy()\n",
        "\n",
        "# Drop TransactionID and isFraud (for regression, we don't use fraud label)\n",
        "df = df.drop(['TransactionID', 'isFraud'], axis=1, errors='ignore')\n",
        "\n",
        "# Select important features for regression\n",
        "important_cols = ['TransactionAmt', 'TransactionDT', 'ProductCD',\n",
        "                  'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
        "                  'addr1', 'addr2', 'dist1', 'dist2',\n",
        "                  'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10',\n",
        "                  'D1', 'D2', 'D3', 'D4', 'D5', 'D10', 'D15',\n",
        "                  'M1', 'M2', 'M3', 'M4',\n",
        "                  'V12', 'V13', 'V20', 'V36', 'V37', 'V45', 'V53', 'V54']\n",
        "\n",
        "# Keep only columns that exist\n",
        "important_cols = [col for col in important_cols if col in df.columns]\n",
        "df = df[important_cols]\n",
        "\n",
        "print(f\"Selected {len(important_cols)} features for modeling\")\n",
        "print(f\"Dataset shape after feature selection: {df.shape}\")\n",
        "\n",
        "# Handle categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "if 'TransactionAmt' in categorical_cols:\n",
        "    categorical_cols.remove('TransactionAmt')\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "print(f\"\\nâœ“ Categorical variables encoded\")\n",
        "\n",
        "# Handle missing values\n",
        "print(f\"\\nMissing values before imputation: {df.isnull().sum().sum()}\")\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "print(f\"Missing values after imputation: {df_imputed.isnull().sum().sum()}\")\n",
        "\n",
        "# Remove outliers using IQR method (optional - keeps extreme values reasonable)\n",
        "Q1 = df_imputed['TransactionAmt'].quantile(0.25)\n",
        "Q3 = df_imputed['TransactionAmt'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 3 * IQR\n",
        "upper_bound = Q3 + 3 * IQR\n",
        "\n",
        "df_clean = df_imputed[(df_imputed['TransactionAmt'] >= lower_bound) & \n",
        "                      (df_imputed['TransactionAmt'] <= upper_bound)]\n",
        "\n",
        "print(f\"\\nData shape after outlier removal: {df_clean.shape}\")\n",
        "print(f\"Removed {len(df_imputed) - len(df_clean)} outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df_clean.drop('TransactionAmt', axis=1)\n",
        "y = df_clean['TransactionAmt']\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\nTraining set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"\\nâœ“ Data split and scaled successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training and Evaluation\n",
        "\n",
        "We will train and evaluate the following regression models:\n",
        "1. **Linear Regression** - Simple baseline model\n",
        "2. **Ridge Regression** - Linear with L2 regularization\n",
        "3. **Lasso Regression** - Linear with L1 regularization\n",
        "4. **Random Forest Regressor** - Ensemble method\n",
        "5. **Gradient Boosting Regressor** - Advanced boosting\n",
        "6. **XGBoost Regressor** - Optimized gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to evaluate regression models\n",
        "def evaluate_regression_model(name, model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train and evaluate a regression model\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print('='*80)\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_r2 = r2_score(y_train, y_pred_train)\n",
        "    test_r2 = r2_score(y_test, y_pred_test)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
        "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
        "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Training Performance:\")\n",
        "    print(f\"   RÂ² Score: {train_r2:.4f}\")\n",
        "    print(f\"   RMSE:     ${train_rmse:.2f}\")\n",
        "    print(f\"   MAE:      ${train_mae:.2f}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Testing Performance:\")\n",
        "    print(f\"   RÂ² Score: {test_r2:.4f}\")\n",
        "    print(f\"   RMSE:     ${test_rmse:.2f}\")\n",
        "    print(f\"   MAE:      ${test_mae:.2f}\")\n",
        "    \n",
        "    # Visualization\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Predicted vs Actual\n",
        "    axes[0].scatter(y_test, y_pred_test, alpha=0.5, s=10)\n",
        "    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "    axes[0].set_xlabel('Actual Transaction Amount ($)')\n",
        "    axes[0].set_ylabel('Predicted Transaction Amount ($)')\n",
        "    axes[0].set_title(f'{name} - Predicted vs Actual', fontweight='bold')\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Residual plot\n",
        "    residuals = y_test - y_pred_test\n",
        "    axes[1].scatter(y_pred_test, residuals, alpha=0.5, s=10)\n",
        "    axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "    axes[1].set_xlabel('Predicted Transaction Amount ($)')\n",
        "    axes[1].set_ylabel('Residuals ($)')\n",
        "    axes[1].set_title(f'{name} - Residual Plot', fontweight='bold')\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return {\n",
        "        'model': name,\n",
        "        'train_r2': train_r2,\n",
        "        'test_r2': test_r2,\n",
        "        'train_rmse': train_rmse,\n",
        "        'test_rmse': test_rmse,\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Linear Regression\n",
        "lr_model = LinearRegression()\n",
        "lr_results = evaluate_regression_model('Linear Regression', lr_model,\n",
        "                                       X_train_scaled, y_train,\n",
        "                                       X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Ridge Regression (L2 Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Ridge Regression\n",
        "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
        "ridge_results = evaluate_regression_model('Ridge Regression', ridge_model,\n",
        "                                         X_train_scaled, y_train,\n",
        "                                         X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 Lasso Regression (L1 Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Lasso Regression\n",
        "lasso_model = Lasso(alpha=0.1, random_state=42, max_iter=10000)\n",
        "lasso_results = evaluate_regression_model('Lasso Regression', lasso_model,\n",
        "                                         X_train_scaled, y_train,\n",
        "                                         X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Random Forest Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Random Forest Regressor\n",
        "rf_model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)\n",
        "rf_results = evaluate_regression_model('Random Forest', rf_model,\n",
        "                                      X_train_scaled, y_train,\n",
        "                                      X_test_scaled, y_test)\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=feature_importance.head(15), x='importance', y='feature', palette='viridis')\n",
        "plt.title('Top 15 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Gradient Boosting Regressor\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
        "gb_results = evaluate_regression_model('Gradient Boosting', gb_model,\n",
        "                                      X_train_scaled, y_train,\n",
        "                                      X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.6 XGBoost Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train XGBoost Regressor\n",
        "xgb_model = XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42)\n",
        "xgb_results = evaluate_regression_model('XGBoost', xgb_model,\n",
        "                                       X_train_scaled, y_train,\n",
        "                                       X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Comparison and Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile all results\n",
        "results_df = pd.DataFrame([lr_results, ridge_results, lasso_results, \n",
        "                           rf_results, gb_results, xgb_results])\n",
        "results_df = results_df.round(4)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# RÂ² Score Comparison\n",
        "axes[0, 0].bar(results_df['model'], results_df['test_r2'], \n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#FFD700'])\n",
        "axes[0, 0].set_title('Model RÂ² Score Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('RÂ² Score')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].set_ylim([0, 1])\n",
        "for i, v in enumerate(results_df['test_r2']):\n",
        "    axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
        "\n",
        "# RMSE Comparison\n",
        "axes[0, 1].bar(results_df['model'], results_df['test_rmse'],\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#FFD700'])\n",
        "axes[0, 1].set_title('Model RMSE Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('RMSE ($)')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(results_df['test_rmse']):\n",
        "    axes[0, 1].text(i, v + 1, f'${v:.2f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# MAE Comparison\n",
        "axes[1, 0].bar(results_df['model'], results_df['test_mae'],\n",
        "               color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#FFD700'])\n",
        "axes[1, 0].set_title('Model MAE Comparison (Test Set)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('MAE ($)')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(results_df['test_mae']):\n",
        "    axes[1, 0].text(i, v + 0.5, f'${v:.2f}', ha='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# Train vs Test RÂ² (Overfitting check)\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "axes[1, 1].bar(x - width/2, results_df['train_r2'], width, label='Train RÂ²', alpha=0.8)\n",
        "axes[1, 1].bar(x + width/2, results_df['test_r2'], width, label='Test RÂ²', alpha=0.8)\n",
        "axes[1, 1].set_title('Train vs Test RÂ² Score', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_ylabel('RÂ² Score')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(results_df['model'], rotation=45, ha='right')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Identify best model\n",
        "best_model_idx = results_df['test_r2'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'model']\n",
        "best_r2 = results_df.loc[best_model_idx, 'test_r2']\n",
        "best_rmse = results_df.loc[best_model_idx, 'test_rmse']\n",
        "best_mae = results_df.loc[best_model_idx, 'test_mae']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ† BEST MODEL SELECTION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(f\"RÂ² Score: {best_r2:.4f}\")\n",
        "print(f\"RMSE: ${best_rmse:.2f}\")\n",
        "print(f\"MAE: ${best_mae:.2f}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nâœ… Regression model training and evaluation completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions and Key Findings\n",
        "\n",
        "### Summary of Results:\n",
        "1. **Objective**: Predict transaction amounts using regression models\n",
        "2. **Models Tested**: Linear, Ridge, Lasso, Random Forest, Gradient Boosting, XGBoost\n",
        "3. **Best Model**: Based on RÂ² Score and RMSE\n",
        "\n",
        "### Key Insights:\n",
        "- **RÂ² Score**: Indicates how well the model explains variance in transaction amounts\n",
        "- **RMSE**: Root Mean Squared Error - penalizes large errors more heavily\n",
        "- **MAE**: Mean Absolute Error - average prediction error in dollars\n",
        "- Tree-based models (Random Forest, Gradient Boosting, XGBoost) typically perform better than linear models for this complex dataset\n",
        "\n",
        "### Model Selection Criteria:\n",
        "- **Highest RÂ² Score**: Better explained variance\n",
        "- **Lowest RMSE/MAE**: More accurate predictions\n",
        "- **Train-Test Gap**: Check for overfitting\n",
        "\n",
        "### Business Applications:\n",
        "1. **Fraud Detection Support**: Unusual predicted vs actual amounts may indicate fraud\n",
        "2. **Revenue Forecasting**: Predict future transaction volumes\n",
        "3. **Risk Assessment**: Identify high-value transactions for additional verification\n",
        "4. **Customer Behavior**: Understand transaction patterns\n",
        "\n",
        "### Future Improvements:\n",
        "1. Feature engineering (time-based features, interaction terms)\n",
        "2. Ensemble methods combining multiple models\n",
        "3. Deep learning approaches (Neural Networks)\n",
        "4. Cross-validation for more robust evaluation\n",
        "5. Hyperparameter tuning for optimization"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
